# AI Service Environment Configuration Template
# Copy this file to .env and customize the values

# Number of requests to process simultaneously
# Higher values = more concurrent processing but more memory usage
OLLAMA_NUM_PARALLEL=20

# Maximum queue size for pending requests
# Higher values = more queuing capacity but more memory usage
OLLAMA_MAX_QUEUE=1000

# NVIDIA GPU Settings (only needed if you have NVIDIA GPU)
# Set to 'none' if you don't have GPU or want to use CPU only
NVIDIA_VISIBLE_DEVICES=all
NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Optional: Additional Ollama Settings
# OLLAMA_HOST=0.0.0.0:11434
# OLLAMA_MODELS=/usr/share/ollama/.ollama/models
# OLLAMA_DEBUG=true